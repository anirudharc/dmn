{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DMN_Plus.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "1bNqK-4TK2cw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### This is the Pytorch implementation of the paper \"Dynamic Memory Networks for Visual and Textual Questions Answering\" . This paper is a much improved version of the previous paper titled \"Dynamic Memory Networks for Natural Language Processing\". The main contribution of this paper is the improved InputModule for calculating the facts from input sentences keeping in mind the exchange of information between input sentences using a Bidirectional GRU and a improved version of MemoryModule using Attention based GRU model. Let's understand the architecure in detail."
      ]
    },
    {
      "metadata": {
        "id": "1RG5e1ZUJjPb",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.3.0.post4-{platform}-linux_x86_64.whl torchvision\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BTvR6aOsJV3s",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch. nn.init as init\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "#from babi_loader import BabiDataset, pad_collate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UwNejzrQNVRx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The QuestionModule is pretty straightforward. It  encodes the question of the task into a distributed vector representation. This representation is fed into the episodic memory module and forms the basis or initial state of the memory state.\n",
        "\n",
        "> Indented block\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "JxWrL6YLJuyJ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "class QuestionModule(nn.Module):\n",
        "\tdef __init__(self, vocab_size, hidden_size):\n",
        "\t\tsuper(QuestionModule, self).__init__()\n",
        "\t\tself.vocab_size = vocab_size # Size of the vocabulary used in word embedding\n",
        "\t\tself.hidden_size = hidden_size # Size of the hidden state of GRU\n",
        "\t\tself.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "\n",
        "\tdef forward(self, questions, word_embedding):\n",
        "\t\t# questions.size() = (batch_size, num_tokens)\n",
        "\t\t# word_embedding -> (batch_size, num_tokens, embedding_length)\n",
        "\t\t# self.gru() -> (1, batch_size, hidden_size)\n",
        "\n",
        "\t\tquestions = word_embedding(questions) # Word embedding of the question\n",
        "\t\toutput, questions = self.gru(questions)\n",
        "\t\tquestions = torch.transpose(questions, 0, 1)\n",
        "\n",
        "\t\treturn questions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j9XVzAckKpHr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "InputModule encodes the raw text inputs into distributed vector representation. We replace the single GRU in DMN (first paper) by two different components. The first component is a sentence reader which encodes the words in a sentence into sentence encoding using a specific encoding scheme called Positional Encoder. THe output of the positional encoder are vectors represented as f1, f2, f3,... and these go as the input to the second component called as Input Fusion Layer. The main function of this layer is to allow the interaction between different input sentences to exchange information not only in the forward direction but also in the backward direction i.e., information from future states flowing backwards using a Bidirectional GRU module. Basically input fusion layer allows for distant supporting sentences to have a more direct interaction."
      ]
    },
    {
      "metadata": {
        "id": "I8CV84bJKBfr",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class InputModule(nn.Module):\n",
        "\tdef __init__(self, vocab_size, hidden_size):\n",
        "\t\tsuper(InputModule, self).__init__()\n",
        "\t\tself.vocab_size = vocab_size\n",
        "\t\tself.hidden_size = hidden_size\n",
        "\t\tself.gru = nn.GRU(hidden_size, hidden_size, bidirectional=True, batch_first=True)\n",
        "\t\tfor name, param in self.gru.state_dict().items():\n",
        "\t\t\tif 'weight' in name:\n",
        "\t\t\t\tinit.xavier_normal(param)\n",
        "\t\tself.dropout = nn.Dropout(0.1)\n",
        "\n",
        "\t''' We will now define the encoding scheme which is positional encoding in the paper \" Dynamic Memory Network for Textual and Visual \n",
        "    Question Answering '''\n",
        "\tdef positional_encoder(embedded_sentence):\n",
        "\t\t# embedded_sentence.size() = (batch_size, num_sentences, num_tokens, embedding_length)\n",
        "\t\t# l.size() = (num_tokems, embedding_length)\n",
        "\t\t# output.size() = (num_batch, num_sentences, embedding_length)\n",
        "\t\t# The outputs are basically f1, f2, f3,.... which will go into the input fusion layer in the next step to add share information\n",
        "\t\t# between sentences using a BiDirfectional GRU module.\n",
        "\n",
        "\t\tbatch_size, num_sentences, num_tokens, embedding_length = embedded_sentence.size()\n",
        "\t\tl = [] # It will be same for all sentences in all batches as num_tokens and embedding_length is same for the entire dataset.\n",
        "\t\tfor j in range(num_tokens):\n",
        "\t\t\tx = []\n",
        "\t\t\tfor d in range(embedding_length):\n",
        "\t\t\t\tx.append((1 - (j/(num_tokens-1))) - (d/(embedding_length-1)) * (1 - 2*j/(num_tokens-1)))\n",
        "\t\t\tl.append(x)\n",
        "\t\t\n",
        "\t\tl = torch.FloatTensor(l)\n",
        "\t\tl = l.unsqueeze(0) # adding an extra dimension at first place for batch_size\n",
        "\t\tl = l.unsqueeze(1) # adding an extra dimension at sencond place for num_sentences\n",
        "\t\tl = l.expand_as(embedded_sentence) # so that l.size() = (batch_size, num_sentences, num_tokens, embedding_length)\n",
        "\n",
        "\t\tmat = embedded_sentence*Variable(l.cuda())\n",
        "\t\tf_ids = torch.sum(mat, dim=2).squeeze(2) # sum along token dimension\n",
        "\n",
        "\t\treturn f_ids\n",
        "\n",
        "\n",
        "\tdef forward(self, input, word_embedding):\n",
        "\t\t# input.size() = (batch_size, num_sentences, num_tokens)\n",
        "\t\t# word_embedding -> (batch_size, num_sentences, num_tokens, embedding_length)\n",
        "\t\t# positional_encoder(word_embedding(input)) -> (batch_size, num_sentences, embedding_length)\n",
        "\t\t# Now BidirectionalGRU blocks receive their input, the output of the positional encoder and finally give facts\n",
        "\t\t# facts.size() = (batch_size, num_sentences, embedding_length) embedding_length = hidden_size\n",
        "\n",
        "\t\tinput = input.view(input.size()[0], -1)\n",
        "\t\tinput = word_embedding(input)\n",
        "\t\tinput = input.view(input.size()[0], input.size()[1], input.size()[2], -1)\n",
        "\t\tinput = self.positional_encoder(input)\n",
        "\t\tinput = self.dropout(input)\n",
        "\n",
        "\t\th0 = Variable(torch.zeros(2, input.size()[0], self.hidden_size).cuda()) # Initializing the initial hidden state (at t=0 time step)\n",
        "\t\tfacts, hdn = self.gru(input, h0)\n",
        "\t\tfacts = facts[:, :, :hidden_size] + facts[:, :, hidden_size:]\n",
        "\n",
        "\t\treturn facts\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rwdciekDPE8p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Attention based GRU is a modification of the original GRU by embedding information from the attention mechanism. In the GRU module, the update gate decides how much of each dimension of the hidden state to retain and how much to update depending upon the input(xi). Since update gate (ui) is calculated using only input(xi) and previous hidden state(hi_1), it certainly lacks any sort of knowledge from the question or the previous memory state. We can use these two to update the hidden state by replacing ui in GRU equation with the gate value(gi_t)"
      ]
    },
    {
      "metadata": {
        "id": "-ZAKnqeoKOMv",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class AttnGRUCell(nn.Module):\n",
        "\tdef __init__(self, input_size, hidden_size):\n",
        "\t\tsuper(AttnGRUCell, self).__init__()\n",
        "\t\tself.input_size = input_size\n",
        "\t\tself.hidden_size = hidden_size\n",
        "\t\tself.Wr = nn.Linear(input_size, hidden_size)\n",
        "\t\tself.Ur = nn.Linear(hidden_size, hidden_size)\n",
        "\t\tself.W = nn.Linear(input_size, hidden_size)\n",
        "\t\tself.U = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "\t\tinit.xavier_normal(self.Wr.state_dict()['weight'])\n",
        "\t\tinit.xavier_normal(self.Ur.state_dict()['weight'])\n",
        "\t\tinit.xavier_normal(self.W.state_dict()['weight'])\n",
        "\t\tinit.xavier_normal(self.U.state_dict()['weight'])\n",
        "\n",
        "\tdef forward(self, fact, hi_1, g):\n",
        "\t\t# fact is the final output of InputModule for each sentence and fact.size() = (batch_size, embedding_length)\n",
        "\t\t# hi_1.size() = (batch_size, embedding_length=hidden_size)\n",
        "\t\t# g.size() = (batch_size, )\n",
        "\n",
        "\t\tr_i = F.sigmoid(self.Wr(fact) + self.Ur(hi_1))\n",
        "\t\th_tilda = F.tanh(self.W(fact) + r*self.U(hi_1))\n",
        "\t\thi = g*h_tilda + (1 - g)*hi_1\n",
        "\n",
        "\t\treturn hi # Returning the next hidden state considering the first fact and so on."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6jlvqcS6QkZN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This class basically update the hidden state using Attention based GRU defined above by iterating over all the sentences. Final hidden state is called the contextual vector which is used to update next memory state."
      ]
    },
    {
      "metadata": {
        "id": "XeCa-szXKPL3",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class AttnGRU(nn.Module):\n",
        "\tdef __init__(self, input_size, hidden_size):\n",
        "\t\tsuper(AttnGRU, self).__init__()\n",
        "\t\tself.input_size = input_size\n",
        "\t\tself.hidden_size = hidden_size\n",
        "\t\tself.AttnGRUCell = AttnGRUCell(input_size, hidden_size)\n",
        "\n",
        "\tdef forward(self, facts, G):\n",
        "\t\t# facts.size() = (batch_size, num_sentences, embedding_length)\n",
        "\t\t# fact.size() = (batch_size, embedding_length=hidden_size)\n",
        "\t\t# G.size() = (batch_size, num_sentences)\n",
        "\t\t# g.size() = (batch_size, )\n",
        "\n",
        "\t\th_0 = Variable(torch.zeros(self.hidden_size)).cuda()\n",
        "\n",
        "\t\tfor sen in range(facts.size()[1]):\n",
        "\t\t\tfact = facts[:, sen, :]\n",
        "\t\t\tg = G[:, sen]\n",
        "\t\t\tif sen == 0: # Initialization for first sentence only \n",
        "\t\t\t\thi_1 = h_0.unsqueeze(0).expand_as(fact)\n",
        "\t\t\thi_1 = self.AttnGRUCell(fact, hi_1, g)\n",
        "\t\tC = hi_1 # Final hidden vector as the contextual vector used for updating memory\n",
        "\n",
        "\t\treturn C"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P-iAOQJgRK39",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The episodic memory module retrieves information from the final input facts by focusing attention on a subset of these facts using gate values. We implement this attention by associating a single scalar value, the attention gate (gi_t) with each fact during each pass. This is calculated by allowing interaction between the fact and both the question and the previous memory state. Once we have the attention gate, we can use an attention mechanism to extract a contextual vector ct which is used to update the memory state. "
      ]
    },
    {
      "metadata": {
        "id": "Rjl7EH9XKSv4",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class MemoryModule(nn.Module): # Takes facts, question and prev_mem as its and output next_mem\n",
        "\tdef __init__(self, hidden_size):\n",
        "\t\tsuper(MemoryModule, self).__init__()\n",
        "\t\tself.hidden_size = hidden_size\n",
        "\t\tself.AttnGRU = AttnGRU(hidden_size, hidden_size)\n",
        "\t\tself.W1 = nn.Linear(4*hidden_size, hidden_size)\n",
        "\t\tself.W2 = nn.Linear(hidden_size, 1)\n",
        "\t\tself.W_mem = nn.Linear(3*hidden_size, hidden_size)\n",
        "\n",
        "\t\tinit.xavier_normal(self.W1.state_dict()['weight'])\n",
        "\t\tinit.xavier_normal(self.W2.state_dict()['weight'])\n",
        "\t\tinit.xavier_normal(self.W_mem.state_dict()['weight'])\n",
        "\n",
        "\tdef gateMatrix(self, facts, questions, prev_mem):\n",
        "\t\t# facts.size() = (batch_size, num_sentences, embedding_length=hidden_size)\n",
        "\t\t# questions.size() = (batch_size, 1, embedding_length)\n",
        "\t\t# prev_mem.size() = (batch_size, 1, embedding_length)\n",
        "\t\t# z.size() = (batch_size, num_sentences, 4*embedding_length)\n",
        "\t\t# G.size() = (batch_size, num_sentences)\n",
        "\n",
        "\t\tquestions = questions.expand_as(facts)\n",
        "\t\tprev_mem = prev_mem.expand_as(facts)\n",
        "\n",
        "\t\tz = torch.cat([facts*questions, facts*prev_mem, torch.abs(facts - questions), torch.abs(facts - prev_mem)], dim=2)\n",
        "\t\t# z.size() = (batch_size, num_sentences, 4*embedding_length)\n",
        "\t\tz = z.view(-1, 4*embedding_length)\n",
        "\t\tZ = self.W2(F.tanh(self.W1(z)))\n",
        "\t\tZ = Z.view(batch_size, -1)\n",
        "\t\tG = F.softmax(Z)\n",
        "\n",
        "\t\treturn G\n",
        "\n",
        "\tdef forward(self, facts, questions, prev_mem):\n",
        "\t\t# questions = questions.unsqueeze(1)\n",
        "\t\t# prev_mem = prev_mem.unsqueeze(1)\n",
        "\t\tG = self.gateMatrix(facts, questions, prev_mem)\n",
        "\t\tC = self.AttnGRU(facts, G)\n",
        "\t\t# Now considering prev_mem, C and question, we will update the memory state as follows\n",
        "\t\tconcat = torch.cat([prev_mem.squeeze(1), C, questions.squeeze(1)], dim=1)\n",
        "\t\tnext_mem = F.relu(self.W_mem(concat))\n",
        "\t\tnext_mem = next_mem.unsqueeze(1)\n",
        "\n",
        "\t\treturn next_mem"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7MknWyyqSYvr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The answer module takes in input both the question as well as final memory hidden state to update the the answer. Here we concatenate the final memory state and the question qnd then we can pass it through a linear layer."
      ]
    },
    {
      "metadata": {
        "id": "ooa5VlVbKWMf",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class AnswerModule(nn.Module):\n",
        "\tdef __init__(self, vocab_size, hidden_size):\n",
        "\t\tsuper(AnswerModule, self).__init__()\n",
        "\t\tself.vocab_size = vocab_size\n",
        "\t\tself.hidden_size = hidden_size\n",
        "\t\tself.W = nn.Linear(2*hidden_size, vocab_size)\n",
        "\t\tinit.xavier_normal(self.W.state_dict()['weight'])\n",
        "\t\tself.dropout = nn.Dropout(0.1)\n",
        "\n",
        "\tdef forward(self, final_mem, questions):\n",
        "\t\tfinal_mem = self.dropout(final_mem)\n",
        "\t\tconcat = torch.cat([final_mem, questions], dim=2).squeeze(1)\n",
        "\t\tout = self.W(concat) # As per the paper, we are concatenating the final memory state m_T, and the question q and passing \n",
        "\t\t# this resultant vector to a linear layer\n",
        "\n",
        "\t\treturn out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VraE9axdQcVV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We define the model for the network incorporating the input, question, answer and the episodic memory module. We use the Cross Entropy loss criterion for measuring loss. Vocab size refers to the size of vocabulary used.\n"
      ]
    },
    {
      "metadata": {
        "id": "2QIzb5DyTIR5",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class DMN(nn.Module):\n",
        "    def __init__(self, hidden_size, vocab_size, num_pass=3, qa=None):\n",
        "        super(DMN,self).__init__()\n",
        "        self.num_pass= num_pass\n",
        "        self.qa= qa\n",
        "        self.word_embedding= nn.Embedding(vocab_size, hidden_size, padding_index=0, sparse=True)\n",
        "        init.uniform(self.word_embedding.state_dict()['weight'], a= -(3**0.5), b=3**0.5)\n",
        "        self.criterion= nn.CrossEntropyLoss(size_average=False)\n",
        "        \n",
        "        self.input_module= input_module(vocab_size,hidden_size)  \n",
        "        self.question_module= question_module(vocab_size, hidden_size) \n",
        "        self.memory= episodic_memory(hidden_size)\n",
        "        self.answer_module= answer_module(vocab_size,hidden_size)\n",
        "        \n",
        "    def forward(self, context, questions):\n",
        "        #facts.size()= (batch_size, num_sentences, embedding_length= hidden.size()) \n",
        "        #questions.size() = (batch_size, 1, embedding_length)\n",
        "        facts= self.input_module(context, self.word_embedding)\n",
        "        questions= self.question_module(questions, self.word_embedding)\n",
        "        X= questions\n",
        "        for passes in range(self.num_pass):\n",
        "            X= self.memory(facts, questions, X)\n",
        "        pred= self.answer_module(X, questions)\n",
        "        return pred_id\n",
        "    \n",
        "    \n",
        "    \n",
        "    def loss(self,context, questions, targets):\n",
        "        output= self.forward(context, questions)\n",
        "        loss= self.criterion(output, targets)\n",
        "        para_loss= 0\n",
        "        for param in self.parameters():\n",
        "            para_loss+= 0.001* torch.sum(param*param)\n",
        "        pred= f.softmax(output)\n",
        "        _, pred_id= torch.max(pred, dim=1)\n",
        "        correct= (pred_id.data == answers.data)\n",
        "        acc= torch.mean(correct.float())   \n",
        "        return loss+reg_loss, acc\n",
        "    \n",
        "    def interpret_indexed_tensor(self,var):\n",
        "        if len(var.size()) == 3:\n",
        "            for n, sentences in enumerate(var):\n",
        "                s= ' '.join([self.qa.IVOCAB[elem.data[0]] for elem in sentence])\n",
        "                print '{n}th batch, {i}th sentence, {s}'\n",
        "                \n",
        "        elif len(var.size()) == 2:\n",
        "            for n, sentence in enumerate(var):\n",
        "                s= ' '.join([self.qa.IVOCAB[elem.data[0]] for elem in sentence])\n",
        "                print '{n}th batch, {s}'\n",
        "                \n",
        "        elif len(var.size()) == 1:\n",
        "            for n, token in enumerate(var):\n",
        "                s= self.qa.IVOCAB[token.data[0]]\n",
        "                print '{n}th of batch, {s}'\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f6uOgp5OSLop",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We now train, validate and test the network on the BABI dataset provided by facebook. Training is done for 256 epochs and we use Adam optimizer for training. Early stopping criterion is employed for training."
      ]
    },
    {
      "metadata": {
        "id": "WBOlJERQKYxh",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    for itr in range(10):\n",
        "        for task in range(1,21):\n",
        "            dataset= BabiDataset(task)\n",
        "            vocab_size= len(dataset.QA.VOCAB)\n",
        "            hidden_size= 80\n",
        "            \n",
        "            model= DMN(hidden_size, vocab_size, num_pass= 3, qa= dataset.QA)   ##vocab_size denotes the size of word embedding used \n",
        "            early_stop_count= 0\n",
        "            early_stop_flag= False\n",
        "            best_acc= 0\n",
        "            optim= torch.optim.Adam(model.parameters())\n",
        "            \n",
        "            for epoch in range(256):\n",
        "                dataset.set_mode('train')\n",
        "                train_load= DataLoader(dataset, batch_size=100, shuffle= True, collate_fn= pad_collate)  ### Loading the babi dataset\n",
        "                \n",
        "                model.train()                                                       ### training the network\n",
        "                if not early_stop_flag:\n",
        "                    total_acc=0\n",
        "                    count= 0\n",
        "                    for batch_id, data in enumerate(train_load):\n",
        "                        optim.zero_grad()\n",
        "                        context, questions, answers = data\n",
        "                        batch_size= context.size()[0]\n",
        "                        context= Variable(context.long())                           ## context.size() = (batch_size, num_sentences, embedding_length) embedding_length = hidden_size \n",
        "                        questions= Variable(questions.long())                       ## questions.size() = (batch_size, num_tokens)\n",
        "                        answers= Variable(answers)\n",
        "                        \n",
        "                        total_loss, acc= model.loss(context,questions,answers)      ## Loss is calculated and gradients are backpropagated through the layers.\n",
        "                        total_loss.backward()\n",
        "                        total_acc+= acc*batch_size\n",
        "                        count+= batch_size\n",
        "                        \n",
        "                        if batch_id %20 == 0:\n",
        "                            print 'task {task_id},epoch {epoch} training loss : {total_loss.data[0]: {10}.{8}},acc : {total_acc/count: {5}.{4}}, batch_id : {batch_id}'\n",
        "                        optim.step()\n",
        "                    \n",
        "                    '''Validation part'''\n",
        "\n",
        "\n",
        "                    dataset.set_mode('valid')\n",
        "                    valid_load = DataLoader(dataset, batch_size= 100, shuffle= False, collate_fn= pad_collate)    ## Loading the validation data\n",
        "                    \n",
        "                    model.eval()\n",
        "                    total_acc=0\n",
        "                    count=0\n",
        "                    for batch_id, data in enumerate(train_load):\n",
        "                        context, questions, answers = data\n",
        "                        batch_size= context.size()[0]\n",
        "                        context= Variable(context.long())\n",
        "                        questions= Variable(questions.long())\n",
        "                        answers= Variable(answers)\n",
        "                        \n",
        "                        _, acc= model.loss(context,questions,answers)  \n",
        "                        total_loss.backward()\n",
        "                        total_acc+= acc*batch_size\n",
        "                        count+= batch_size\n",
        "                    \n",
        "                    total_acc= total_acc/ count\n",
        "                    \n",
        "                    if total_acc > best_acc:\n",
        "                        best_acc= total_acc\n",
        "                        best_state= model.state_dict()\n",
        "                        early_stop_count= 0\n",
        "                    else:\n",
        "                        early_stop_count+= 1                   \n",
        "                        if early_stop_count > 20:\n",
        "                            early_Stop_flag= True\n",
        "                    \n",
        "                    print 'itr {itr}, task {task_id}, Epoch {epoch} validation accuracy : {total_acc: {5}.{4}}'\n",
        "                    with open('log.txt', 'a') as fp:\n",
        "                        fp.write('itr {itr}, task {task_id}, Epoch {epoch} validation accuracy : {total_acc: {5}.{4}}' + '\\n')\n",
        "                    if total_acc == 1.0:\n",
        "                        break\n",
        "                else:\n",
        "                    print(f'itr {itr}, task {task_id} Early Stopping at Epoch {epoch}, validation accuracy : best_acc: {5}.{4}')\n",
        "            \n",
        "            \n",
        "            '''Testing part'''\n",
        "            \n",
        "            dataset.set_mode('test')\n",
        "            test_load= DataLoader(dataset, batch_size=100, shuffle= False, collate_fn= pad_collate)\n",
        "            \n",
        "            test_acc= 0\n",
        "            count=0\n",
        "            \n",
        "            for batch_id, data in enumerate(test_load):\n",
        "                    context, questions, answers = data\n",
        "                    batch_size= context.size()[0]\n",
        "                    context= Variable(context.long())\n",
        "                    questions= Variable(questions.long())\n",
        "                    answers= Variable(answers)\n",
        "                    \n",
        "                    model.load_state_dict(best_state)\n",
        "                    _, acc= model.loss(context, questions, answers)\n",
        "                    \n",
        "                    test_acc += acc* batch_size \n",
        "                    count += batch_size\n",
        "                    print('itr {itr}, task {task_id}, Epoch {epoch} test accuracy : test_acc / count: {5}.{4}')\n",
        "                    \n",
        "                    os.makedirs('models', exist_ok= True)\n",
        "                    with open('models/task{task_id}_epoch{epoch}_itr{itr}_acc{test_acc/count}.pth', 'wb') as fp:\n",
        "                        torch.save(model.state_dict(), fp)\n",
        "                    with open('log.txt', 'a') as fp:\n",
        "                        fp.write('itr {itr}, task {task_id}, Epoch {epoch}] test accuracy : {total_acc: {5}.{4}}' + '\\n')\n",
        "                        "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}